{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from qumedl.mol.encoding.selfies_ import Selfies\n",
    "from qumedl.models.transformer.pat import CausalMolPAT\n",
    "from qumedl.models.transformer.loss_functions import compute_transformer_loss\n",
    "from qumedl.training.collator import TensorBatchCollator\n",
    "from qumedl.training.tensor_batch import TensorBatch\n",
    "from qumedl.models.activations import NewGELU\n",
    "from qumedl.models.priors import GaussianPrior\n",
    "from orquestra.drug.discovery.docking.utils import process_molecule\n",
    "from orquestra.drug.discovery.validator.filter_abstract import FilterAbstract\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# from qumedl.models.priors import QCBMPrior\n",
    "from orquestra.drug.discovery.validator import (\n",
    "    GeneralFilter,\n",
    "    PainFilter,\n",
    "    WehiMCFilter,\n",
    "    SybaFilter,\n",
    ")\n",
    "from orquestra.drug.metrics import MoleculeNovelty, get_diversity\n",
    "from orquestra.drug.utils import ConditionFilters\n",
    "import wandb  # Import wandb\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cloudpickle\n",
    "\n",
    "## RBM\n",
    "import optax\n",
    "from orquestra.qml.models.rbm.jx import RBM\n",
    "from orquestra.qml.api import Batch\n",
    "\n",
    "# Initialize Qiskit Runtime Service with specific credentials\n",
    "import pickle\n",
    "\n",
    "\n",
    "class TartarusFilters(FilterAbstract):\n",
    "    def apply(self, smile: str):\n",
    "        _, status = process_molecule(smile)\n",
    "        if status == \"PASS\":\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def save_object(obj, filename):\n",
    "    \"\"\"Save a Python object to a file using pickle.\"\"\"\n",
    "    with open(filename, \"wb\") as file:  # Open the file in write-binary mode\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "\n",
    "def load_object(filename):\n",
    "    \"\"\"Load a Python object from a pickle file.\"\"\"\n",
    "    with open(filename, \"rb\") as file:  # Open the file in read-binary mode\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "class RBMModel(RBM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_visible: int,\n",
    "        n_hidden: int,\n",
    "        random_seed=32,\n",
    "        optimizer=optax.sgd(learning_rate=1e-6),\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_visible, n_hidden, random_seed=random_seed, optimizer=optimizer\n",
    "        )\n",
    "        self.num_qubits = self.n_visible\n",
    "\n",
    "    def train(self, data, probs, n_epoch):\n",
    "        rbm_batch = Batch(data=data, probs=probs)\n",
    "        # rbm_batch.batch_size = -1\n",
    "        all_resuls = []\n",
    "        for i in range(n_epoch):\n",
    "            all_resuls.append(self._train_on_batch(rbm_batch))\n",
    "        return all_resuls\n",
    "\n",
    "\n",
    "# save in file:\n",
    "def save_obj(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        r = cloudpickle.dump(obj, f)\n",
    "    return r\n",
    "\n",
    "\n",
    "def load_obj(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        obj = cloudpickle.load(f)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_project_log_folder(project_name=\"pat\"):\n",
    "    # Generate a project name based on the current date\n",
    "    current_date = datetime.now()\n",
    "    # datetime.today().strftime(\"%Y_%d_%mT%H_%M_%S.%f\")\n",
    "    project_name = current_date.strftime(f\"{project_name}_%Y-%m-%d_%H-%M-%S.%f\")\n",
    "    project_today = current_date.strftime(f\"{project_name}_%Y-%m-%d\")\n",
    "\n",
    "    # Define the path for the logs directory\n",
    "    logs_dir_path = \"./logs\"\n",
    "\n",
    "    # Check if the logs directory exists, if not create it\n",
    "    if not os.path.exists(logs_dir_path):\n",
    "        os.makedirs(logs_dir_path)\n",
    "\n",
    "    # Define the path for the new project directory within the logs folder\n",
    "    project_dir_path = os.path.join(logs_dir_path, project_name)\n",
    "\n",
    "    # Check if the project directory exists, if not create it\n",
    "    if not os.path.exists(project_dir_path):\n",
    "        os.makedirs(project_dir_path)\n",
    "\n",
    "    print(f\"Project log folder created at: {project_dir_path}\")\n",
    "    return (project_dir_path, project_name, project_today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no input\n",
      "prior_name:rbm,prior_size:16,DEVICE:cuda,cuda_device_code:4,dataset_arg:tartarus,random_seed:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")  # needs to be cuda on the cluster\n",
    "\n",
    "print(\"no input\")\n",
    "prior_name = \"rbm\"\n",
    "prior_size = 16  # int(sys.argv[2])\n",
    "random_seed = 0\n",
    "cuda_device_code = \"4\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_device_code\n",
    "dataset_arg = \"tartarus\"\n",
    "backend_sim = True\n",
    "wandb_active = False\n",
    "\n",
    "print(\n",
    "    f\"prior_name:{prior_name},prior_size:{prior_size},DEVICE:{DEVICE},cuda_device_code:{cuda_device_code},dataset_arg:{dataset_arg},random_seed:{random_seed}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = 'cpu'\n",
    "batch_size = 1024\n",
    "prior_dim = prior_size\n",
    "\n",
    "model_dim = embedding_dim = 256  # should be embedding_dim/n_attn_heads\n",
    "n_attn_heads = 8\n",
    "n_encoder_layers = 4\n",
    "\n",
    "n_g_samples = 5000\n",
    "\n",
    "dropout = 0.2\n",
    "\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "min_learning_rate = 1e-6\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "n_epochs_prior = 30\n",
    "n_test_samples = 5000\n",
    "\n",
    "dataset_name = \"/root/generative-models/scripts/data/docking_hill_climbing_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = dataset_name.split(\".\")[0]\n",
    "if os.path.isfile(f\"{pickle_name}.pkl\"):\n",
    "    selfies = load_object(f\"{pickle_name}.pkl\")\n",
    "else:\n",
    "    selfies = Selfies.from_smiles_csv(dataset_name)\n",
    "    save_object(selfies, f\"{pickle_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qumedl.mol.encoding.selfies_.Selfies at 0x7f38270d2d50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selfies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "smiles_dataset_df = pd.read_csv(dataset_name)\n",
    "smiles_dataset = smiles_dataset_df.smiles.to_list()\n",
    "\n",
    "selfies_dataset = selfies.as_dataset()\n",
    "\n",
    "dl_shuffler = torch.Generator()\n",
    "dl_shuffler.manual_seed(random_seed)\n",
    "\n",
    "\n",
    "if prior_name == \"random\":\n",
    "    prior = GaussianPrior(dim=prior_dim)\n",
    "    prior_trainable = False\n",
    "elif prior_name == \"rbm\":\n",
    "    prior = RBMModel(\n",
    "        n_visible=prior_dim,\n",
    "        n_hidden=2 * prior_dim,\n",
    "        random_seed=random_seed,\n",
    "        optimizer=optax.sgd(learning_rate=1e-6),\n",
    "    )\n",
    "    prior_trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CausalMolPAT(\n",
    "    vocab_size=selfies.n_tokens,\n",
    "    embedding_dim=embedding_dim,\n",
    "    prior_dim=prior.num_qubits,\n",
    "    model_dim=model_dim,\n",
    "    n_attn_heads=n_attn_heads,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    hidden_act=NewGELU(),\n",
    "    dropout=dropout,\n",
    "    padding_token_idx=selfies.pad_index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samll transformer\n",
    "# wandb_project_name = \"pat_2024-08-16_02-09-32.570627_2024-08-16\"\n",
    "# log_path = \"/root/generative-models/scripts/logs/pat_2024-08-16_02-09-32.570627/model_epoch_99.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big trasformer model\n",
    "wand_project_name = \"pat_2024-08-16_15-22-18.083128_2024-08-16\"\n",
    "# # /root/generative-models/scripts/logs/pat_2024-08-16_15-22-18.083128/model_epoch_99.pt\n",
    "log_path = \"/root/generative-models/scripts/logs/pat_2024-08-16_15-22-18.083128/model_epoch_99.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     # Wrap the model with nn.DataParallel\n",
    "#     model = nn.DataParallel(model)\n",
    "#     batch_size = batch_size * torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalMolPAT(\n",
       "  (embedding): Embedding(110, 256, padding_idx=13)\n",
       "  (projectx_addy): ProjectXxY(\n",
       "    (_projection): Linear(in_features=16, out_features=256, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (repeat): Identity()\n",
       "  (pe): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (activation): NewGELU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=110, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def remove_prefix_from_state_dict(state_dict, prefix=\"_orig_mod.\"):\n",
    "    \"\"\"Remove a prefix from all keys in the state_dict.\"\"\"\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(prefix):\n",
    "            new_key = k[len(prefix) :]  # Remove the prefix\n",
    "            new_state_dict[new_key] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "# Load the state_dict\n",
    "state_dict = torch.load(log_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# Remove the prefix\n",
    "state_dict = remove_prefix_from_state_dict(state_dict)\n",
    "\n",
    "# Load the modified state_dict into your model\n",
    "# model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tokens = torch.full(\n",
    "    (n_g_samples, 1),\n",
    "    fill_value=selfies.start_index,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.int,\n",
    ")\n",
    "prior_samples = torch.tensor(\n",
    "    np.asarray(prior.generate(n_g_samples, random_seed=random_seed))\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalMolPAT(\n",
       "  (embedding): Embedding(110, 256, padding_idx=13)\n",
       "  (projectx_addy): ProjectXxY(\n",
       "    (_projection): Linear(in_features=16, out_features=256, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (repeat): Identity()\n",
       "  (pe): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (projection): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "        (activation): NewGELU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=110, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits 16\n",
      "unique prior 994\n",
      "unique molecules 930\n",
      "total molecules 994\n",
      "diversity 0.9356136820925554\n"
     ]
    }
   ],
   "source": [
    "n_samples = 1000\n",
    "temperature = 1e-9\n",
    "# create an array of random bits of shape (n_samples, bits)\n",
    "torch.manual_seed(1260)\n",
    "prior_samples = torch.randint(0, 2, (n_samples, prior_size)).float()\n",
    "# keep unique prior samples:\n",
    "prior_samples = torch.unique(prior_samples, dim=0)\n",
    "# move model and samples to GPU:\n",
    "start_tokens = torch.full(\n",
    "    (len(prior_samples), 1),\n",
    "    fill_value=selfies.start_index,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.int,\n",
    ")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "prior_samples = prior_samples.to(DEVICE)\n",
    "# generate molecules from the prior samples\n",
    "\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    generated = model.module.generate(\n",
    "        start_tokens,\n",
    "        prior_samples,\n",
    "        max_new_tokens=selfies.max_length,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "else:\n",
    "    generated = model.generate(\n",
    "        start_tokens,\n",
    "        prior_samples,\n",
    "        max_new_tokens=selfies.max_length,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "test_molecules = selfies.decode(generated.cpu().numpy())\n",
    "ligands = selfies.selfie_to_smiles(test_molecules)\n",
    "print(\"bits\", prior_size)\n",
    "print(\"unique prior\", len(set(prior_samples)))\n",
    "print(\"unique molecules\", len(set(ligands)))\n",
    "print(\"total molecules\", len(ligands))\n",
    "print(\"diversity\", len(set(ligands)) / len(ligands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits 16\n",
      "unique prior 994\n",
      "unique molecules 956\n",
      "total molecules 994\n",
      "diversity 0.9617706237424547\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.01\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    generated = model.module.generate(\n",
    "        start_tokens,\n",
    "        prior_samples,\n",
    "        max_new_tokens=selfies.max_length,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "else:\n",
    "    generated = model.generate(\n",
    "        start_tokens,\n",
    "        prior_samples,\n",
    "        max_new_tokens=selfies.max_length,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "test_molecules = selfies.decode(generated.cpu().numpy())\n",
    "ligands = selfies.selfie_to_smiles(test_molecules)\n",
    "print(\"bits\", prior_size)\n",
    "print(\"unique prior\", len(set(prior_samples)))\n",
    "print(\"unique molecules\", len(set(ligands)))\n",
    "print(\"total molecules\", len(ligands))\n",
    "print(\"diversity\", len(set(ligands)) / len(ligands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity: 1.0\n",
      "Novelty: 92.05298013245033\n"
     ]
    }
   ],
   "source": [
    "# Load the test molecules from the JSON file\n",
    "smiles_dataset_df = pd.read_csv(dataset_name)\n",
    "with open(\"/root/generative-models/scripts/logs/pat_2024-09-27_00-15-43.541538/test_molecules-99.json\", \"r\") as file:\n",
    "    test_molecules = json.load(file)\n",
    "\n",
    "# Decode the molecules\n",
    "smiles_dataset = smiles_dataset_df.smiles.to_list()\n",
    "ligands = selfies.selfie_to_smiles(test_molecules)\n",
    "ligands = [ligand for ligand in set(ligands) if ligand]\n",
    "# Compute diversity\n",
    "diversity = len(set(ligands)) / len(ligands)\n",
    "\n",
    "# Compute novelty\n",
    "novelty = MoleculeNovelty(smiles_dataset)\n",
    "novelty_score = novelty.get_novelity_smiles(ligands,threshold=0.6)\n",
    "\n",
    "print(\"Diversity:\", diversity)\n",
    "print(\"Novelty:\", novelty_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diversity: 1.0\n",
      "Novelty: 85.03575076608784\n"
     ]
    }
   ],
   "source": [
    "# Load the test molecules from the JSON file\n",
    "smiles_dataset_df = pd.read_csv(dataset_name)\n",
    "with open(\"/root/generative-models/scripts/logs/pat_2024-09-27_00-15-43.541538/test_molecules-399.json\", \"r\") as file:\n",
    "    test_molecules = json.load(file)\n",
    "\n",
    "# Decode the molecules\n",
    "smiles_dataset = smiles_dataset_df.smiles.to_list()\n",
    "ligands = selfies.selfie_to_smiles(test_molecules)\n",
    "ligands = [ligand for ligand in set(ligands) if ligand]\n",
    "# Compute diversity\n",
    "diversity = len(set(ligands)) / len(ligands)\n",
    "\n",
    "# Compute novelty\n",
    "novelty = MoleculeNovelty(smiles_dataset)\n",
    "novelty_score = novelty.get_novelity_smiles(ligands,threshold=0.6)\n",
    "\n",
    "print(\"Diversity:\", diversity)\n",
    "print(\"Novelty:\", novelty_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47.22222222222222"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novelty_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.979"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ligands)/2000"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
